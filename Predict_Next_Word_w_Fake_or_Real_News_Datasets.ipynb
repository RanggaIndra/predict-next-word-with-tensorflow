{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsMBUEj1-km6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_df = pd.read_csv('fake_or_real_news.csv')"
      ],
      "metadata": {
        "id": "dMIYhkpj_XFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = list(text_df.text.values)\n",
        "joined_text = \" \".join(text)"
      ],
      "metadata": {
        "id": "j_whHbxPB6-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_text = joined_text[:200000]"
      ],
      "metadata": {
        "id": "KR1yJVmMFQR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "tokens = tokenizer.tokenize(partial_text.lower())"
      ],
      "metadata": {
        "id": "yYp42iBAFntg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = np.unique(tokens)\n",
        "unique_tokens_index = {token: idx for idx, token in enumerate(unique_tokens)}"
      ],
      "metadata": {
        "id": "eZKhQVPcF5qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = 10\n",
        "input_words = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(tokens) - n_words):\n",
        "  input_words.append(tokens[i:i + n_words])\n",
        "  next_words.append(tokens[i + n_words])"
      ],
      "metadata": {
        "id": "kL6nnQ-cGMHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)\n",
        "y = np.zeros((len(next_words), len(unique_tokens)), dtype=bool)"
      ],
      "metadata": {
        "id": "TJCap07VG_aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, words in enumerate(input_words):\n",
        "  for j, word in enumerate(words):\n",
        "    X[i, j, unique_tokens_index[word]] = 1\n",
        "  y[i, unique_tokens_index[next_words[i]]] = 1"
      ],
      "metadata": {
        "id": "V8-vbEE8IB8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(unique_tokens)))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "GfqJ7wB_IsRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01), metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=128, epochs=30, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbhZxAjwJWRu",
        "outputId": "47d1fde0-64f2-473a-ea33-45f87022064e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "267/267 [==============================] - 9s 18ms/step - loss: 7.0560 - accuracy: 0.0564\n",
            "Epoch 2/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 6.8293 - accuracy: 0.0650\n",
            "Epoch 3/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 6.6151 - accuracy: 0.0752\n",
            "Epoch 4/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 6.3746 - accuracy: 0.0938\n",
            "Epoch 5/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 6.1416 - accuracy: 0.1100\n",
            "Epoch 6/30\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 5.9006 - accuracy: 0.1263\n",
            "Epoch 7/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 5.6453 - accuracy: 0.1420\n",
            "Epoch 8/30\n",
            "267/267 [==============================] - 5s 19ms/step - loss: 5.3607 - accuracy: 0.1631\n",
            "Epoch 9/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 5.0749 - accuracy: 0.1881\n",
            "Epoch 10/30\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 4.7815 - accuracy: 0.2167\n",
            "Epoch 11/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 4.4863 - accuracy: 0.2484\n",
            "Epoch 12/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 4.1990 - accuracy: 0.2793\n",
            "Epoch 13/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 3.9122 - accuracy: 0.3159\n",
            "Epoch 14/30\n",
            "267/267 [==============================] - 5s 19ms/step - loss: 3.6588 - accuracy: 0.3494\n",
            "Epoch 15/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 3.4093 - accuracy: 0.3864\n",
            "Epoch 16/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 3.1792 - accuracy: 0.4193\n",
            "Epoch 17/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 2.9520 - accuracy: 0.4558\n",
            "Epoch 18/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 2.7384 - accuracy: 0.4937\n",
            "Epoch 19/30\n",
            "267/267 [==============================] - 5s 19ms/step - loss: 2.5541 - accuracy: 0.5286\n",
            "Epoch 20/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 2.3594 - accuracy: 0.5663\n",
            "Epoch 21/30\n",
            "267/267 [==============================] - 4s 16ms/step - loss: 2.1837 - accuracy: 0.6023\n",
            "Epoch 22/30\n",
            "267/267 [==============================] - 6s 21ms/step - loss: 2.0271 - accuracy: 0.6339\n",
            "Epoch 23/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 1.8660 - accuracy: 0.6700\n",
            "Epoch 24/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 1.7274 - accuracy: 0.7008\n",
            "Epoch 25/30\n",
            "267/267 [==============================] - 5s 18ms/step - loss: 1.5918 - accuracy: 0.7322\n",
            "Epoch 26/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 1.4665 - accuracy: 0.7590\n",
            "Epoch 27/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 1.3428 - accuracy: 0.7898\n",
            "Epoch 28/30\n",
            "267/267 [==============================] - 5s 19ms/step - loss: 1.2189 - accuracy: 0.8158\n",
            "Epoch 29/30\n",
            "267/267 [==============================] - 5s 17ms/step - loss: 1.1121 - accuracy: 0.8368\n",
            "Epoch 30/30\n",
            "267/267 [==============================] - 4s 17ms/step - loss: 1.0156 - accuracy: 0.8574\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x798b81113bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('predict-next-word-model')"
      ],
      "metadata": {
        "id": "52nX6mQhJwgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('predict-next-word-model')"
      ],
      "metadata": {
        "id": "2PT_rEpYUq_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(input_text, text_length, creativity=3):\n",
        "  word_sequence = input_text.split()\n",
        "  current = 0\n",
        "  for _ in range(text_length):\n",
        "    sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
        "    try:\n",
        "      choice = unique_tokens[random.choice(predict_next_word(sub_sequence, creativity))]\n",
        "    except:\n",
        "      choice = random.choice(unique_tokens)\n",
        "    word_sequence.append(choice)\n",
        "    current += 1\n",
        "  return \" \".join(word_sequence)"
      ],
      "metadata": {
        "id": "0ST7qjOZRWsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Mr president will go to some of the country that\", 100, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "_n8SqTGmRpdo",
        "outputId": "811ebeed-fa5f-45f7-9f6e-d7494e6e138b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mr president will go to some of the country that launchers slogans weiner lofty catch all bias 1990 japan quickly promptly operating idiot failed factors define mind goods continue demons taliban lousy cutting sunni early rajan posts realclearpolitics troop debate chart enriching board oregon discouraged readiness right snubs bush computers cheer stress called discovered collars rice dec dad newsletter exacerbated absolute pointed voices truly wholly guns tycho pooh projecting flight disposal revolt majority fuelled commonality scared version puzzle night dan vs wings helicopters miss ethnicity convention nine further pt professional grinding flags stepping unnoticed ronald angry marked filed suggesting rest lewandowski comprehend edged dismissed prevalence lousy adopting closely drawing executives'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}